{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Echo State Network for Artificial Grammar Learning(Reber)\n",
    "\n",
    "# Importing required modules\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "from ipywidgets import *\n",
    "from IPython.display import *\n",
    "\n",
    "# Generating Reber Strings\n",
    "\n",
    "import random as rnd\n",
    "\n",
    "class ReberGrammarLexicon(object):\n",
    "\n",
    "    lexicon = set() #contain Reber words\n",
    "    graph = [ [(1,'T'), (2,'P')], \\\n",
    "            [(1, 'S'), (3, 'X')], \\\n",
    "            [(2,'T') ,(4, 'V')],  \\\n",
    "            [(2, 'X'), (5,'S')],           \\\n",
    "            [(3, 'P'),(5, 'V')],  \\\n",
    "            [(6,'E')] ]  #store the graph\n",
    "\n",
    "    def __init__(self, num, maxSize = 1000): #fill Lexicon with num words\n",
    "\n",
    "        self.maxSize = maxSize\n",
    "\n",
    "        if maxSize < 5:\n",
    "            raise NameError('maxSize too small, require maxSize > 4') \n",
    "\n",
    "        while len(self.lexicon) < num:\n",
    "\n",
    "            word = self.generateWord()\n",
    "            if word != None:\n",
    "                self.lexicon.add(word)\n",
    "\n",
    "    def generateWord(self): #generate one word\n",
    "\n",
    "        c = 2\n",
    "        currentEdge = 0\n",
    "        word = 'B'\n",
    "\n",
    "        while c <= self.maxSize:\n",
    "            \n",
    "            if(((currentEdge==3) | (currentEdge==4)) & (c<(self.maxSize/9))):\n",
    "                inc=0\n",
    "            else:\n",
    "                inc = rnd.randint(0,len(self.graph[currentEdge])-1)\n",
    "            \n",
    "            nextEdge = self.graph[currentEdge][inc][0]\n",
    "            word += self.graph[currentEdge][inc][1]\n",
    "            currentEdge = nextEdge\n",
    "            if currentEdge == 6 :\n",
    "                break\n",
    "            c+=1\n",
    "\n",
    "        if c > self.maxSize :\n",
    "            return None\n",
    "\n",
    "        return word\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = ['B', 'E', 'P', 'S', 'T', 'V', 'X']\n",
    "values = np.array(data)\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit(integer_encoded)\n",
    "\n",
    "def func(word,start=0):\n",
    "    word = np.array(list(word[start:]))\n",
    "    encoded = label_encoder.transform(word)\n",
    "    return onehot_encoded.transform(encoded.reshape(-1,1))    \n",
    "\n",
    "def preprocessing(dictionary,start=0):\n",
    "    length = len(dictionary)\n",
    "    result = func(list(dictionary)[0])\n",
    "    \n",
    "    for word in list(dictionary)[1:]:\n",
    "        b = func(word)\n",
    "        result = np.vstack((result,b))\n",
    "        \n",
    "        \n",
    "    return result     \n",
    "\n",
    "generator = ReberGrammarLexicon(2000,maxSize=20)\n",
    "\n",
    "generator.lexicon\n",
    "\n",
    "training_set = preprocessing(list(generator.lexicon)[:2000])\n",
    "\n",
    "training_set.shape\n",
    "\n",
    "# Setting random seed\n",
    "\n",
    "def set_seed(seed=None):\n",
    "    \"\"\"Making the seed (for random values) variable if None\"\"\"\n",
    "\n",
    "    if seed is None:\n",
    "        import time\n",
    "        seed = int((time.time()*10**6) % 4294967295)\n",
    "        print(seed)\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "        print(\"Seed used for random values:\", seed)\n",
    "    except:\n",
    "        print(\"!!! WARNING !!!: Seed was not set correctly.\")\n",
    "    return seed\n",
    "\n",
    "# Creating Network Class\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, trainLen=2000, testLen=2000, initLen=100) :\n",
    "        self.initLen = initLen\n",
    "        self.trainLen = trainLen\n",
    "        self.testLen = testLen\n",
    "        self.data = training_set\n",
    "        self.inSize = self.outSize = 7 #Input/Output dimensions\n",
    "        self.resSize = 400 #Reservoir size (prediction)\n",
    "        #self.resSize = 1000 #Reservoir size (generation)\n",
    "        self.a = 1 #Leak rate alpha\n",
    "        self.spectral_radius = 1.25 #Spectral raidus\n",
    "        self.input_scaling = 1. #Input scaling\n",
    "        self.reg =  1e-8 #None #Regularization factor - if None,\n",
    "        #we'd use pseudo-inverse rather than ridge regression\n",
    "\n",
    "        self.mode = 'prediction'\n",
    "        #self.mode = 'generative'\n",
    "\n",
    "        #Change the seed, reservoir performances should be averaged accross\n",
    "        #at least 20 random instances (with the same set of parameters)\n",
    "        seed = None #42\n",
    "\n",
    "        set_seed(seed)\n",
    "        \n",
    "nw = Network()\n",
    "\n",
    "# Generating Win,W randomly and then generating X,Ytarget \n",
    "\n",
    "from scipy.sparse import rand\n",
    "\n",
    "def initialization(nw) :\n",
    "\n",
    "    #Weights\n",
    "    #nw.Win = (np.random.rand(nw.resSize,1+nw.inSize)) * nw.input_scaling\n",
    "    #nw.W = np.random.rand(nw.resSize,nw.resSize) \n",
    "    nw.Win = np.array(rand(nw.resSize,1+nw.inSize, density=0.25, format=\"csr\", random_state=42).todense())\n",
    "    nw.W = np.array(rand(nw.resSize,nw.resSize, density=0.25, format=\"csr\", random_state=42).todense())\n",
    "    \n",
    "    #Matrices\n",
    "    #Allocated memory for the design (collected states) matrix\n",
    "    nw.X = np.zeros((1+nw.inSize+nw.resSize,nw.trainLen-nw.initLen))\n",
    "    #Set the corresponding target matrix directly\n",
    "    nw.Ytarget = nw.data[None,nw.initLen+1:nw.trainLen+1]\n",
    "\n",
    "    #Run the reservoir with the data and collect X\n",
    "    nw.x = np.zeros((nw.resSize,1))  \n",
    "    \n",
    "    return(nw)\n",
    "\n",
    "# Computing spectral radius(biggest of the absolute eigen values of W matrix) and then scaling W matrix using that\n",
    "\n",
    "def compute_spectral_radius(nw):\n",
    "    print('Computing spectral radius...',end=\" \")\n",
    "    rhoW = max(abs(linalg.eig(nw.W)[0]))\n",
    "    print('Done.')\n",
    "    nw.W *= nw.spectral_radius / rhoW\n",
    "    \n",
    "    return(nw)\n",
    "\n",
    "# Learning phase\n",
    "\n",
    "# ùë•ùëõ = (1‚àíùõº)ùë•ùëõ +  ùõºtanh(ùëäùëñùëõ.ùë¢ùëõ ‚àí 1) + ùëä.ùë•ùëõ‚àí1\n",
    "\n",
    "def learning_phase(nw) :\n",
    "    for t in range(nw.trainLen):\n",
    "        #Input data\n",
    "        nw.u = nw.data[t]\n",
    "        \n",
    "      #  if (nw.u == np.array((0, 1, 0, 0, 0, 0, 0))).all(): \n",
    "      #      nw.x = np.zeros((nw.resSize,1))\n",
    "      #  else:\n",
    "        nw.x = (1-nw.a)*nw.x + nw.a*np.tanh( np.dot(nw.Win, np.vstack((1,nw.u.reshape(7,1))) ) + np.dot( nw.W, nw.x ) )\n",
    "        #After the initialization, we start modifying X\n",
    "        if t >= nw.initLen:\n",
    "            nw.X[:,t-nw.initLen] = np.vstack((1,nw.u.reshape(7,1),nw.x.reshape(nw.resSize,1)))[:,0]\n",
    "            \n",
    "    return(nw)\n",
    "\n",
    "# Training output weights using ridge regression\n",
    "# ùëäùëúùë¢ùë° = (ùëåùë°.ùëãùëá) . (ùëã.ùëãùëá+ùëüùëíùëî.ùêº)^-1\n",
    "\n",
    "def train_output(nw) :\n",
    "    nw.X_T = nw.X.T\n",
    "    if nw.reg is not None:\n",
    "        # Ridge regression (linear regression with regularization)\n",
    "        nw.Wout = np.dot(np.dot(nw.Ytarget[0].T,nw.X_T), linalg.inv(np.dot(nw.X,nw.X_T) + \\\n",
    "            nw.reg*np.eye(1+nw.inSize+nw.resSize) ) )\n",
    "    else:\n",
    "        # Pseudo-inverse\n",
    "        nw.Wout = np.dot(nw.Ytarget, linalg.pinv(nw.X) )\n",
    "        \n",
    "    return(nw)\n",
    "\n",
    "# Testing in a particular mode\n",
    "\n",
    "def test(nw) :\n",
    "    #Run the trained ESN in a generative mode. no need to initialize here, \n",
    "    #because x is initialized with training data and we continue from there.\n",
    "    nw.Y = np.zeros((nw.testLen,nw.outSize))\n",
    "    nw.u = nw.data[nw.trainLen]\n",
    "    nw.reservoir = np.zeros((nw.testLen,nw.resSize))\n",
    "    for t in range(nw.testLen):\n",
    "#        if (nw.u == np.array((0, 1, 0, 0, 0, 0, 0))).all(): \n",
    "#            nw.x = np.zeros((nw.resSize,1))\n",
    "#        else:\n",
    "        nw.x = (1-nw.a)*nw.x + nw.a*np.tanh( np.dot(nw.Win, np.vstack((1,nw.u.reshape(7,1))) ) + np.dot( nw.W, nw.x ) )\n",
    "        \n",
    "        nw.reservoir[t] = nw.x.reshape(nw.resSize,)\n",
    "        nw.y = np.dot(nw.Wout, np.vstack((1,nw.u.reshape(7,1),nw.x.reshape(nw.resSize,1))) )\n",
    "        nw.Y[t][:] = nw.y.reshape(1,7)\n",
    "        if nw.mode == 'generative':\n",
    "            #Generative mode:\n",
    "            nw.u = nw.y\n",
    "        elif nw.mode == 'prediction':\n",
    "            #Predictive mode:\n",
    "            nw.u = nw.data[nw.trainLen+t+1] \n",
    "        else:\n",
    "            raise(Exception, \"ERROR: 'mode' was not set correctly.\")\n",
    "    \n",
    "    return(nw)\n",
    "\n",
    "def compute_error(nw) :\n",
    "    # Computing MSE for the first errorLen iterations\n",
    "    errorLen = 500\n",
    "    mse = sum( np.square( nw.data[nw.trainLen+1:nw.trainLen+errorLen+1] - nw.Y[0,0:errorLen] ) ) / errorLen\n",
    "    print('MSE = ' + str( mse ))\n",
    "    \n",
    "    return(nw)\n",
    "\n",
    "def compute_network(nw) :\n",
    "    nw = initialization(nw)\n",
    "    nw = compute_spectral_radius(nw)\n",
    "    nw = learning_phase(nw)\n",
    "    nw = train_output(nw)\n",
    "    nw = test(nw)  \n",
    "    nw = compute_error(nw)\n",
    "    return(nw)\n",
    "\n",
    "# Definition of the network parameters\n",
    "\n",
    "select_mode = ToggleButtons(description='Mode:',\n",
    "    options=['prediction', 'generative'])\n",
    "var1 = FloatSlider(value=300, min=0, max=1000, step=1, description='resSize')\n",
    "var2 = FloatSlider(value=100, min=0, max=2000, step=1, description='initLen')\n",
    "var3 = FloatSlider(value=2000, min=0, max=30000, step=1, description='trainLen')\n",
    "var4 = FloatSlider(value=2000, min=0, max=8000, step=1, description='testLen')\n",
    "var5 = FloatSlider(value=1.25, min=0, max=10, step=0.05, description='spectral radius')\n",
    "var6 = FloatSlider(value=0.3, min=0, max=1, step=0.01, description='leak rate')\n",
    "valid = Button(description='Validate')\n",
    "\n",
    "def record_values(_) :\n",
    "    clear_output()\n",
    "    nw.mode=select_mode.value\n",
    "    nw.resSize=int(var1.value)\n",
    "    nw.initLen=int(var2.value)\n",
    "    nw.trainLen=int(var3.value)\n",
    "    nw.testLen=int(var4.value)\n",
    "    nw.spectral_radius=float(var5.value)\n",
    "    nw.a=float(var6.value)\n",
    "    print(\"InitLen:\", nw.initLen, \"TrainLen:\", nw.trainLen, \"TestLen:\", nw.testLen) \n",
    "    print(\"ResSize:\", nw.resSize, \"Spectral Radius:\", nw.spectral_radius, \"Leak Rate:\", nw.a)\n",
    "    compute_network(nw)\n",
    "    return(nw)\n",
    "\n",
    "display(select_mode)\n",
    "display(var1)\n",
    "display(var2)\n",
    "display(var3)\n",
    "display(var4)\n",
    "display(var5)\n",
    "display(var6)\n",
    "display(valid)\n",
    "\n",
    "valid.on_click(record_values)\n",
    "\n",
    "y_pred = nw.Y\n",
    "\n",
    "y_test = training_set[nw.trainLen+1:nw.trainLen+nw.testLen+1]\n",
    "\n",
    "def top_2_accuracy(y_test,y_pred):\n",
    "    k = 0\n",
    "    for i in range(nw.testLen):\n",
    "        if (y_test[i].argmax() == y_pred[i].argsort()[-1]) | (y_test[i].argmax() == y_pred[i].argsort()[-2]):\n",
    "            k += 1\n",
    "    print(k/nw.testLen)        \n",
    "\n",
    "top_2_accuracy(y_test,y_pred)\n",
    "\n",
    "chars='BEPSTVX'\n",
    "\n",
    "y_test[:20]\n",
    "\n",
    "y_pred[:20].round(2)\n",
    "\n",
    "from plotly import __version__\n",
    "import cufflinks as cf\n",
    "from plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10).fit(nw.reservoir)\n",
    "\n",
    "df = pd.DataFrame({'Number of Components':list(range(1,nw.resSize+1)),'Cummulative Variance Explained':list(np.cumsum(pca.explained_variance_ratio_))})\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "df.iplot(kind='scatter',x='Number of Components',y='Cummulative Variance Explained')\n",
    "\n",
    "np.cumsum(pca.explained_variance_ratio_)[:20]\n",
    "\n",
    "reduced_internal_representations = pca.transform(nw.reservoir)\n",
    "\n",
    "reduced_internal_representations[0].round(2)\n",
    "\n",
    "corpus = generator.lexicon\n",
    "\n",
    "def label_generator(corpus):\n",
    "    result = []\n",
    "    \n",
    "    for word in corpus:\n",
    "        \n",
    "        for i in range(len(word)):\n",
    "            result.append(word[:i+1])\n",
    "            \n",
    "    return result        \n",
    "\n",
    "labels = label_generator(corpus)\n",
    "test_labels = labels[nw.trainLen+1:nw.trainLen+nw.testLen+1]\n",
    "\n",
    "len(test_labels)\n",
    "\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "dendrogram = sch.dendrogram(sch.linkage(reduced_internal_representations[:40],method='ward'),orientation='right',labels=test_labels[:40])\n",
    "\n",
    "nw.Wout.shape\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=5)\n",
    "\n",
    "lsa = svd.fit_transform(nw.Wout)\n",
    "\n",
    "lsa\n",
    "\n",
    "print(svd.explained_variance_ratio_)\n",
    "\n",
    "print(svd.singular_values_)\n",
    "\n",
    "from numpy.linalg import svd as SVD\n",
    "\n",
    "U, S, VT = SVD(nw.Wout,full_matrices=False)\n",
    "\n",
    "print(\"Left Singular Vectors:\")\n",
    "print(U)\n",
    "print()\n",
    "print(\"Singular Values:\") \n",
    "print(np.diag(S))\n",
    "print()\n",
    "print(\"Right Singular Vectors:\") \n",
    "print(VT)\n",
    "\n",
    "# check that this is an exact decomposition\n",
    "# @ is used for matrix multiplication in Py3, use np.matmul with Py2\n",
    "print(U @ np.diag(S) @ VT)\n",
    "\n",
    "chars='BEPSTVX'\n",
    "print(list(chars))\n",
    "print(np.round(y_pred,decimals=2)[:20])\n",
    "\n",
    "y_test[:20]\n",
    "\n",
    "# Graph 1: Plotting neurons activations (total)\n",
    "\n",
    "var10 = FloatSlider(value=2000,min=10,max=nw.trainLen-nw.initLen,step=10,description='time steps')\n",
    "var11 = FloatSlider(value=10, min=1, max=nw.resSize, step=1, description='number of neurons')\n",
    "valid = Button(description='Validate')\n",
    "\n",
    "def trace_graph3(_) :\n",
    "    clear_output()\n",
    "    f=int(var10.value)\n",
    "    nb=int(var11.value)\n",
    "    plt.figure(3).clear()\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.plot( nw.X[2:2+nb,0:f].T )\n",
    "    print(nw.X.shape)\n",
    "    plt.ylim([-1.1,1.1])\n",
    "    plt.title('Activations $\\mathbf{x}(n)$ from Reservoir Neurons ID 0 to '+str(nb-1)+' for '+str(f)+' time steps')\n",
    "    \n",
    "valid.on_click(trace_graph3)\n",
    "    \n",
    "display(var10)\n",
    "display(var11)\n",
    "display(valid)\n",
    "\n",
    "# Graph 2: Plotting single neuron activation\n",
    "\n",
    "var12 = FloatSlider(value=2000,min=10,max=nw.trainLen-nw.initLen,step=10,description='time steps')\n",
    "var13 = FloatSlider(value=2, min=0, max=nw.resSize-1, step=1, description='neuron ID')\n",
    "valid = Button(description='Validate')\n",
    "\n",
    "def trace_graph4(_) :\n",
    "    clear_output()\n",
    "    f=int(var12.value)\n",
    "    num=int(var13.value)\n",
    "    plt.figure(4).clear()\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot( nw.X[2+num,:f].T )\n",
    "    plt.ylim([-1.1,1.1])\n",
    "    plt.title('Activations $\\mathbf{x}(n)$ from Reservoir Neuron ID '+str(num)+' for '+str(f)+' time steps')\n",
    "\n",
    "valid.on_click(trace_graph4)\n",
    "\n",
    "display(var12)\n",
    "display(var13)\n",
    "display(valid)\n",
    "\n",
    "# Graph 3: Output weights at the end of the simulation\n",
    "\n",
    "valid = Button(description='Show')\n",
    "\n",
    "def trace_graph5(_) :\n",
    "    clear_output()\n",
    "    plt.figure(5).clear()\n",
    "    plt.figure(figsize=(12,7))\n",
    "    plt.bar(range(1+nw.inSize+nw.resSize), np.squeeze(nw.Wout.T) )\n",
    "    plt.title('Output weights $\\mathbf{W}^{out}$')\n",
    "\n",
    "valid.on_click(trace_graph5)\n",
    "\n",
    "display(valid)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
